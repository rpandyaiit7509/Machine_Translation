{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGI1vEXjHFSR",
        "outputId": "40e8afca-9de0-49a8-c243-79571fadab1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.7710\n",
            "Epoch 100, Loss: 0.5653\n",
            "Epoch 200, Loss: 0.5650\n",
            "Epoch 300, Loss: 0.5650\n",
            "Epoch 400, Loss: 0.5650\n",
            "Epoch 500, Loss: 0.5649\n",
            "Epoch 600, Loss: 0.5649\n",
            "Epoch 700, Loss: 0.5649\n",
            "Epoch 800, Loss: 0.5649\n",
            "Epoch 900, Loss: 0.5649\n",
            "Kannada: naanu ge shale hoguttene\n",
            "English: i go to school\n",
            "\n",
            "Kannada: naavu ahara tinnuttene\n",
            "English: we eat food\n",
            "\n",
            "Kannada: naanu pustaka oduttene\n",
            "English: i read book\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "d_model = 16  # Reduced from 64\n",
        "n_heads = 2   # Reduced from 4\n",
        "d_ff = 32     # Reduced from 128\n",
        "max_seq_len = 10\n",
        "vocab_size_en = 12\n",
        "vocab_size_ka = 12\n",
        "learning_rate = 0.001\n",
        "warmup_steps = 400\n",
        "epochs = 1000\n",
        "label_smoothing = 0.1\n",
        "\n",
        "# Scaled Dot-Product Attention\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        d_k = Q.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        return output, attention_weights\n",
        "\n",
        "# Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.attention = ScaledDotProductAttention()\n",
        "        # Xavier initialization\n",
        "        nn.init.xavier_uniform_(self.W_q.weight)\n",
        "        nn.init.xavier_uniform_(self.W_k.weight)\n",
        "        nn.init.xavier_uniform_(self.W_v.weight)\n",
        "        nn.init.xavier_uniform_(self.W_o.weight)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "        Q = self.W_q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        output, attn_weights = self.attention(Q, K, V, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.W_o(output)\n",
        "\n",
        "# Feed-Forward Network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        nn.init.xavier_uniform_(self.linear1.weight)\n",
        "        nn.init.xavier_uniform_(self.linear2.weight)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "# Positional Encoding\n",
        "def positional_encoding(max_seq_len, d_model):\n",
        "    pe = torch.zeros(max_seq_len, d_model)\n",
        "    position = torch.arange(0, max_seq_len, dtype=torch.float32).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-np.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pe\n",
        "\n",
        "# Layer Normalization\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, epsilon=1e-6):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta\n",
        "\n",
        "# Transformer Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.mha(x, x, x, mask)\n",
        "        x = self.norm1(x + attn_output)\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + ffn_output)\n",
        "        return x\n",
        "\n",
        "# Transformer Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, n_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "        self.norm3 = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        attn1 = self.mha1(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + attn1)\n",
        "        attn2 = self.mha2(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + attn2)\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm3(x + ffn_output)\n",
        "        return x\n",
        "\n",
        "# Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, max_seq_len, vocab_size_en, vocab_size_ka):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderLayer(d_model, n_heads, d_ff)\n",
        "        self.decoder = DecoderLayer(d_model, n_heads, d_ff)\n",
        "        self.pos_enc = positional_encoding(max_seq_len, d_model)\n",
        "        self.embedding_en = nn.Embedding(vocab_size_en, d_model)\n",
        "        self.embedding_ka = nn.Embedding(vocab_size_ka, d_model)\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size_ka)\n",
        "        nn.init.xavier_uniform_(self.embedding_en.weight)\n",
        "        nn.init.xavier_uniform_(self.embedding_ka.weight)\n",
        "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        src_emb = self.embedding_en(src) + self.pos_enc[:src.size(1), :].to(src.device)\n",
        "        tgt_emb = self.embedding_ka(tgt) + self.pos_enc[:tgt.size(1), :].to(tgt.device)\n",
        "        enc_output = self.encoder(src_emb, src_mask)\n",
        "        dec_output = self.decoder(tgt_emb, enc_output, src_mask, tgt_mask)\n",
        "        return self.output_layer(dec_output)\n",
        "\n",
        "# Create Masks\n",
        "def create_padding_mask(seq, pad_id=0):\n",
        "    return (seq != pad_id).unsqueeze(1).unsqueeze(2).long()\n",
        "\n",
        "def create_causal_mask(seq_len):\n",
        "    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(1)\n",
        "    return mask\n",
        "\n",
        "# Synthetic Dataset\n",
        "vocab_en = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"i\": 3, \"go\": 4, \"to\": 5, \"school\": 6, \"read\": 7, \"book\": 8, \"we\": 9, \"eat\": 10, \"food\": 11}\n",
        "vocab_ka = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"naanu\": 3, \"hoguttene\": 4, \"ge\": 5, \"shale\": 6, \"oduttene\": 7, \"pustaka\": 8, \"naavu\": 9, \"tinnuttene\": 10, \"ahara\": 11}\n",
        "data = [\n",
        "    ([1, 3, 5, 6, 4, 2], [1, 3, 4, 5, 6, 2]),  # Naanu ge shale hoguttene -> I go to school\n",
        "    ([1, 3, 8, 7, 2], [1, 3, 7, 8, 2]),        # Naanu pustaka oduttene -> I read book\n",
        "    ([1, 9, 11, 10, 2], [1, 9, 10, 11, 2]),    # Naavu ahara tinnuttene -> We eat food\n",
        "    ([1, 3, 11, 10, 2], [1, 3, 10, 11, 2]),    # Naanu ahara tinnuttene -> I eat food\n",
        "    ([1, 9, 5, 6, 4, 2], [1, 9, 4, 5, 6, 2]),  # Naavu ge shale hoguttene -> We go to school\n",
        "    ([1, 3, 5, 6, 7, 2], [1, 3, 7, 5, 6, 2]),  # Naanu ge shale oduttene -> I read to school\n",
        "    ([1, 9, 8, 7, 2], [1, 9, 7, 8, 2]),        # Naavu pustaka oduttene -> We read book\n",
        "    ([1, 3, 8, 4, 2], [1, 3, 4, 8, 2]),        # Naanu pustaka hoguttene -> I go book\n",
        "    ([1, 9, 5, 6, 7, 2], [1, 9, 7, 5, 6, 2]),  # Naavu ge shale oduttene -> We read to school\n",
        "]\n",
        "\n",
        "\n",
        "# Cross-Entropy Loss with Label Smoothing\n",
        "class CrossEntropyLossWithSmoothing(nn.Module):\n",
        "    def __init__(self, vocab_size, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        output = output.view(-1, self.vocab_size)\n",
        "        target = target.view(-1)\n",
        "        log_probs = torch.log_softmax(output, dim=-1)\n",
        "        confidence = 1 - self.smoothing\n",
        "        label_smoothed = torch.full_like(log_probs, self.smoothing / (self.vocab_size - 1))\n",
        "        label_smoothed.scatter_(1, target.unsqueeze(1), confidence)\n",
        "        label_smoothed[target == 0] = 0  # Ignore padding\n",
        "        return -torch.mean(torch.sum(label_smoothed * log_probs, dim=-1))\n",
        "\n",
        "# Learning Rate Schedule\n",
        "class TransformerScheduler:\n",
        "    def __init__(self, optimizer, d_model, warmup_steps):\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.step_num = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_num += 1\n",
        "        lr = (self.d_model ** -0.5) * min(self.step_num ** -0.5, self.step_num * self.warmup_steps ** -1.5)\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        return lr\n",
        "\n",
        "# Training\n",
        "# model = Transformer(d_model, n_heads, d_ff, max_seq_len, len(vocab_en), len(vocab_ka))\n",
        "\n",
        "model = Transformer(d_model, n_heads, d_ff, max_seq_len, len(vocab_ka), len(vocab_en))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = TransformerScheduler(optimizer, d_model, warmup_steps)\n",
        "criterion = CrossEntropyLossWithSmoothing(vocab_size_ka, label_smoothing)\n",
        "\n",
        "def train(model, data, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for src, tgt in data:\n",
        "            src = torch.tensor([src], dtype=torch.long)\n",
        "            tgt_in = torch.tensor([tgt[:-1]], dtype=torch.long)\n",
        "            tgt_out = torch.tensor([tgt[1:]], dtype=torch.long)\n",
        "            src_mask = create_padding_mask(src)\n",
        "            tgt_mask = create_causal_mask(tgt_in.size(1))\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt_in, src_mask, tgt_mask)\n",
        "            loss = criterion(output, tgt_out)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {total_loss / len(data):.4f}\")\n",
        "\n",
        "train(model, data, epochs)\n",
        "\n",
        "\n",
        "\n",
        "# translate_ka_to_en function\n",
        "def translate_ka_to_en(model, sentence, vocab_ka, vocab_en, max_len=10):\n",
        "    model.eval()\n",
        "    inv_vocab_en = {v: k for k, v in vocab_en.items()}\n",
        "    tokens = [\"<sos>\"] + sentence.lower().split() + [\"<eos>\"]\n",
        "    src = [vocab_ka.get(t, 0) for t in tokens]\n",
        "    src = torch.tensor([src + [0] * (max_len - len(src))], dtype=torch.long)\n",
        "    src_mask = create_padding_mask(src)\n",
        "    tgt = torch.tensor([[vocab_en[\"<sos>\"]] + [0] * (max_len - 1)], dtype=torch.long)\n",
        "\n",
        "    for i in range(max_len - 1):\n",
        "        tgt_mask = create_causal_mask(i + 1)\n",
        "        with torch.no_grad():\n",
        "            output = model(src, tgt[:, :i + 1], src_mask, tgt_mask)\n",
        "        next_token = output[0, i, :].argmax().item()\n",
        "        tgt[0, i + 1] = next_token\n",
        "        if next_token == vocab_en[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    return \" \".join(inv_vocab_en[t.item()] for t in tgt[0] if t.item() != 0 and t.item() != vocab_en[\"<sos>\"] and t.item() != vocab_en[\"<eos>\"])\n",
        "\n",
        "# Test\n",
        "test_sentences_ka = [\n",
        "    \"naanu ge shale hoguttene\",\n",
        "    \"naavu ahara tinnuttene\",\n",
        "    \"naanu pustaka oduttene\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences_ka:\n",
        "    translated = translate_ka_to_en(model, sentence, vocab_ka, vocab_en)\n",
        "    print(f\"Kannada: {sentence}\")\n",
        "    print(f\"English: {translated}\\n\")\n"
      ]
    }
  ]
}