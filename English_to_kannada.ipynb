{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports & Hyperparameters"
      ],
      "metadata": {
        "id": "IOlKsZPfw267"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters Configuration\n",
        "\n",
        "d_model = 16            # Dimension of model (hidden size for embeddings and layers)\n",
        "n_heads = 2             # Number of attention heads in Multi-Head Attention\n",
        "d_ff = 32               # Dimension of feedforward layer inside Transformer block\n",
        "max_seq_len = 40        # Maximum length of input/output sequence\n",
        "vocab_size_en = 12      # Size of English vocabulary (input language)\n",
        "vocab_size_ka = 12      # Size of Kannada vocabulary (target language)\n",
        "learning_rate = 0.001   # Starting learning rate for optimizer\n",
        "warmup_steps = 400      # Steps for learning rate warm-up schedule, defines the number of warm-up steps during training, and its function is to gradually increase the learning rate from a small value to the target (or base) learning rate over the first 400 steps of training.\n",
        "epochs = 1000           # Number of training epochs\n",
        "label_smoothing = 0.1   # Amount of smoothing in cross-entropy loss to prevent overfitting\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ikU3nEs3w34p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Attention Mechanism\n",
        "\n",
        "Scaled Dot-Product Attention\n"
      ],
      "metadata": {
        "id": "bpU1JCjKw4aR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The attention mechanism computes attention using the following formula:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xdV_MhGz09os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaled Dot-Product Attention\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()  # Initialize the parent class (nn.Module)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        d_k = Q.size(-1)  # Get the dimensionality of the query vectors (last dimension)\n",
        "\n",
        "        # Compute the raw attention scores by taking the dot product of Q and K^T,\n",
        "        # then scale by the square root of d_k for numerical stability\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "\n",
        "        # If a mask is provided (e.g., to ignore certain positions like padding),\n",
        "        # replace those positions with a very large negative value so their softmax becomes ~0\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to obtain normalized attention weights\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Multiply the attention weights with the value matrix V to get the final output\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        # Return both the output and the attention weights (for analysis or visualization)\n",
        "        return output, attention_weights\n"
      ],
      "metadata": {
        "id": "tAqJhLLSw4yY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-Head Attention\n",
        "\n",
        "\n",
        "*  Splits the embedding space into  Q, K, V are projected into subspaces.\n",
        "*   Attention is applied per head.\n",
        "*   Results are concatenated and linearly transformed."
      ],
      "metadata": {
        "id": "iapEK7g3w7RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model                # Total dimensionality of the model\n",
        "        self.n_heads = n_heads                # Number of attention heads\n",
        "        self.d_k = d_model // n_heads         # Dimensionality of each head (must divide evenly)\n",
        "\n",
        "        # Linear projections for queries, keys, and values (one for each input type)\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linear layer to combine the output from all heads\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Scaled dot-product attention module (shared across heads)\n",
        "        self.attention = ScaledDotProductAttention()\n",
        "\n",
        "        # Initialize weights using Xavier initialization for better training stability\n",
        "        nn.init.xavier_uniform_(self.W_q.weight)\n",
        "        nn.init.xavier_uniform_(self.W_k.weight)\n",
        "        nn.init.xavier_uniform_(self.W_v.weight)\n",
        "        nn.init.xavier_uniform_(self.W_o.weight)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)  # Get the batch size\n",
        "\n",
        "        # Project Q, K, V and reshape to (batch_size, n_heads, seq_len, d_k)\n",
        "        Q = self.W_q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Apply scaled dot-product attention on each head in parallel\n",
        "        output, attn_weights = self.attention(Q, K, V, mask)\n",
        "\n",
        "        # Concatenate all the heads' outputs and reshape to (batch_size, seq_len, d_model)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # Final linear layer to project back to d_model dimensions\n",
        "        return self.W_o(output)\n"
      ],
      "metadata": {
        "id": "22KgcPgfw532"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed-Forward Network"
      ],
      "metadata": {
        "id": "cBaNSjBs90CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed-Forward Network (Position-wise Feed-Forward Layer used in Transformer blocks)\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "\n",
        "        # First linear layer projects from d_model to d_ff (hidden layer dimension)\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "\n",
        "        # Second linear layer projects back from d_ff to d_model\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "        # Initialize both linear layers using Xavier initialization for better convergence\n",
        "        nn.init.xavier_uniform_(self.linear1.weight)\n",
        "        nn.init.xavier_uniform_(self.linear2.weight)\n",
        "\n",
        "        # ReLU activation function for non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply linear1 -> ReLU -> linear2 in sequence\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n"
      ],
      "metadata": {
        "id": "k8D-h3tb9zhf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding and Layer Normalization"
      ],
      "metadata": {
        "id": "Pz-dXtTKw9yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Positional Encoding\n",
        "\n",
        "* Adds position information to token embeddings using sine (even indices) and cosine (odd indices) functions:\n",
        "*   Helps Transformer understand the order of tokens.\n",
        "*   Generates a fixed pattern based on position and model dimension.\n",
        "\n",
        "\n",
        "## Layer Normalization\n",
        "*   Makes training more stable.\n",
        "\n",
        "\\\\begin{equation}\n",
        "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
        "\\end{equation}\n",
        "\n",
        "*   γ and β are learnable parameters (scale and shift)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uXphLVaG3BC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Positional encoding for sequence positions\n",
        "def positional_encoding(max_seq_len, d_model):\n",
        "    # Create a matrix of zeros to hold positional encodings\n",
        "    pe = torch.zeros(max_seq_len, d_model)\n",
        "\n",
        "    # Create a tensor of shape (max_seq_len, 1) with position indices [0, 1, 2, ..., max_seq_len-1]\n",
        "    position = torch.arange(0, max_seq_len, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # Compute the denominator term for sine and cosine (logarithmic scale frequencies)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-np.log(10000.0) / d_model))\n",
        "\n",
        "    # Apply sine to even indices (0, 2, 4, ...)\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "    # Apply cosine to odd indices (1, 3, 5, ...)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    # Return the positional encoding matrix of shape (max_seq_len, d_model)\n",
        "    return pe\n",
        "\n",
        "\n",
        "# Custom Layer Normalization\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, epsilon=1e-6):\n",
        "        super().__init__()\n",
        "\n",
        "        # Learnable scale parameter initialized to 1s\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "        # Learnable shift parameter initialized to 0s\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "\n",
        "        # Small value to avoid division by zero in normalization\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute the mean across the last dimension\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "\n",
        "        # Compute the standard deviation across the last dimension\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "\n",
        "        # Normalize the input, then scale and shift using gamma and beta\n",
        "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta"
      ],
      "metadata": {
        "id": "jHmFncs6w-Vf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Encoder Layer\n"
      ],
      "metadata": {
        "id": "7w4oDYAw2irb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single layer of Transformer Encoder\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head self-attention layer\n",
        "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "        # Position-wise feed-forward network\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "\n",
        "        # Layer normalization after attention sub-layer\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "\n",
        "        # Layer normalization after feed-forward sub-layer\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Apply multi-head self-attention (Q=K=V=x for self-attention)\n",
        "        attn_output = self.mha(x, x, x, mask)\n",
        "\n",
        "        # Add & Norm: residual connection + layer normalization\n",
        "        x = self.norm1(x + attn_output)\n",
        "\n",
        "        # Apply position-wise feed-forward network\n",
        "        ffn_output = self.ffn(x)\n",
        "\n",
        "        # Add & Norm: residual connection + layer normalization\n",
        "        x = self.norm2(x + ffn_output)\n",
        "\n",
        "        # Output of the encoder layer\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "bIzEzOku2jUk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Decoder Layer"
      ],
      "metadata": {
        "id": "3F2qaX5D2j5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single layer of Transformer Decoder\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "\n",
        "        # First Multi-Head Attention: masked self-attention for the target sequence\n",
        "        self.mha1 = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "        # Second Multi-Head Attention: encoder-decoder attention (cross-attention)\n",
        "        self.mha2 = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "        # Feed-forward network applied after attention blocks\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "\n",
        "        # Layer normalizations after each sub-layer\n",
        "        self.norm1 = LayerNorm(d_model)  # After self-attention\n",
        "        self.norm2 = LayerNorm(d_model)  # After encoder-decoder attention\n",
        "        self.norm3 = LayerNorm(d_model)  # After feed-forward network\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        # Step 1: Masked self-attention on the decoder input\n",
        "        # (prevents the decoder from seeing future tokens)\n",
        "        attn1 = self.mha1(x, x, x, tgt_mask)\n",
        "\n",
        "        # Add & Norm after self-attention\n",
        "        x = self.norm1(x + attn1)\n",
        "\n",
        "        # Step 2: Encoder-decoder (cross) attention\n",
        "        # Queries come from decoder, Keys and Values from encoder output\n",
        "        attn2 = self.mha2(x, enc_output, enc_output, src_mask)\n",
        "\n",
        "        # Add & Norm after cross-attention\n",
        "        x = self.norm2(x + attn2)\n",
        "\n",
        "        # Step 3: Position-wise feed-forward network\n",
        "        ffn_output = self.ffn(x)\n",
        "\n",
        "        # Add & Norm after FFN\n",
        "        x = self.norm3(x + ffn_output)\n",
        "\n",
        "        # Return the output of the decoder layer\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "jw-wVr7S2kbQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model"
      ],
      "metadata": {
        "id": "amrzxKPHw5hQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Transformer model class\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, max_seq_len, vocab_size_en, vocab_size_ka):\n",
        "        super().__init__()  # Initialize the parent nn.Module class\n",
        "\n",
        "        # Encoder layer with multi-head attention and feed-forward\n",
        "        self.encoder = EncoderLayer(d_model, n_heads, d_ff)\n",
        "\n",
        "        # Decoder layer with masked multi-head attention, encoder-decoder attention, and feed-forward\n",
        "        self.decoder = DecoderLayer(d_model, n_heads, d_ff)\n",
        "\n",
        "        # Positional encoding to capture word order information\n",
        "        self.pos_enc = positional_encoding(max_seq_len, d_model)\n",
        "\n",
        "        # Embedding layer for English input tokens\n",
        "        self.embedding_en = nn.Embedding(vocab_size_en, d_model)\n",
        "\n",
        "        # Embedding layer for Kannada target tokens\n",
        "        self.embedding_ka = nn.Embedding(vocab_size_ka, d_model)\n",
        "\n",
        "        # Output linear layer to project decoder output to vocabulary size\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size_ka)\n",
        "\n",
        "        # Initialize embedding and output weights using Xavier uniform for better convergence\n",
        "        nn.init.xavier_uniform_(self.embedding_en.weight)\n",
        "        nn.init.xavier_uniform_(self.embedding_ka.weight)\n",
        "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        # Apply embedding and positional encoding to source sequence\n",
        "        src_emb = self.embedding_en(src) + self.pos_enc[:src.size(1), :].to(src.device)\n",
        "\n",
        "        # Apply embedding and positional encoding to target sequence\n",
        "        tgt_emb = self.embedding_ka(tgt) + self.pos_enc[:tgt.size(1), :].to(tgt.device)\n",
        "\n",
        "        # Pass through encoder with optional source mask\n",
        "        enc_output = self.encoder(src_emb, src_mask)\n",
        "\n",
        "        # Pass through decoder with encoder output and optional masks\n",
        "        dec_output = self.decoder(tgt_emb, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        # Apply output linear layer to get logits over Kannada vocabulary\n",
        "        return self.output_layer(dec_output)\n"
      ],
      "metadata": {
        "id": "S1iWF1sHr8Bk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create padding mask to avoid attending to <PAD> tokens (usually ID = 0)\n",
        "def create_padding_mask(seq, pad_id=0):\n",
        "    # Create a mask where True (1) indicates non-pad tokens and False (0) indicates pad tokens\n",
        "    # Shape: (batch_size, seq_len)\n",
        "    mask = (seq != pad_id)\n",
        "\n",
        "    # Unsqueeze to shape (batch_size, 1, 1, seq_len) to broadcast correctly in attention computation\n",
        "    return mask.unsqueeze(1).unsqueeze(2).long()\n",
        "    # This ensures that attention doesn't consider pad tokens during attention scoring\n",
        "\n",
        "\n",
        "# Create a causal mask to prevent decoder from looking ahead (future positions)\n",
        "def create_causal_mask(seq_len):\n",
        "    # Create a lower triangular matrix (1s in lower triangle, 0s elsewhere)\n",
        "    # This allows each position to attend only to itself and previous positions\n",
        "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
        "\n",
        "    # Add batch and head dimensions for broadcasting during attention\n",
        "    return mask.unsqueeze(0).unsqueeze(1)\n",
        "    # Shape: (1, 1, seq_len, seq_len) to be used with decoder self-attention\n",
        "\n"
      ],
      "metadata": {
        "id": "EFi1_uNMr-g7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Synthetic Vocabulary and Dataset"
      ],
      "metadata": {
        "id": "4P0a9DdJsgnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# English vocabulary with word to index mapping\n",
        "vocab_en = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"i\": 3, \"go\": 4, \"to\": 5, \"school\": 6, \"read\": 7, \"book\": 8, \"we\": 9, \"eat\": 10, \"food\": 11}\n",
        "\n",
        "# Kannada vocabulary with word to index mapping\n",
        "vocab_ka = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"naanu\": 3, \"hoguttene\": 4, \"ge\": 5, \"shale\": 6, \"oduttene\": 7, \"pustaka\": 8, \"naavu\": 9, \"tinnuttene\": 10, \"ahara\": 11}\n",
        "\n",
        "# Parallel English–Kannada sentence pairs encoded as token IDs\n",
        "data = [\n",
        "    ([1, 3, 4, 5, 6, 2], [1, 3, 5, 6, 4, 2]),  # \"i go to school\" -> \"naanu ge shale hoguttene\"\n",
        "    ([1, 3, 7, 8, 2], [1, 3, 8, 7, 2]),        # \"i read book\" -> \"naanu pustaka oduttene\"\n",
        "    ([1, 9, 10, 11, 2], [1, 9, 11, 10, 2]),    # \"we eat food\" -> \"naavu ahara tinnuttene\"\n",
        "    ([1, 3, 10, 11, 2], [1, 3, 11, 10, 2]),    # \"i eat food\" -> \"naanu ahara tinnuttene\"\n",
        "    ([1, 9, 4, 5, 6, 2], [1, 9, 5, 6, 4, 2]),  # \"we go to school\" -> \"naavu ge shale hoguttene\"\n",
        "    ([1, 3, 7, 5, 6, 2], [1, 3, 5, 6, 7, 2]),  # \"i read to school\" -> \"naanu ge shale oduttene\"\n",
        "    ([1, 9, 7, 8, 2], [1, 9, 8, 7, 2]),        # \"we read book\" -> \"naavu pustaka oduttene\"\n",
        "    ([1, 3, 4, 8, 2], [1, 3, 8, 4, 2]),        # \"i go book\" -> \"naanu pustaka hoguttene\"\n",
        "    ([1, 9, 7, 5, 6, 2], [1, 9, 5, 6, 7, 2]),  # \"we read to school\" -> \"naavu ge shale oduttene\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "7y4dkD-0sC-o"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cross-Entropy Loss with Label Smoothing"
      ],
      "metadata": {
        "id": "WnwMyaAFst1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom loss function using label smoothing to improve generalization\n",
        "class CrossEntropyLossWithSmoothing(nn.Module):\n",
        "    def __init__(self, vocab_size, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        output = output.view(-1, self.vocab_size)     # Reshape output to (batch*seq_len, vocab_size)\n",
        "        target = target.view(-1)                      # Flatten target to (batch*seq_len)\n",
        "        log_probs = torch.log_softmax(output, dim=-1) # Compute log softmax probabilities\n",
        "\n",
        "        # Set smoothing values (confidence for correct class and small probability for others)\n",
        "        confidence = 1 - self.smoothing\n",
        "        label_smoothed = torch.full_like(log_probs, self.smoothing / (self.vocab_size - 1))\n",
        "\n",
        "        # Replace correct class probabilities with confidence\n",
        "        label_smoothed.scatter_(1, target.unsqueeze(1), confidence)\n",
        "\n",
        "        # Ignore loss where target is padding\n",
        "        label_smoothed[target == 0] = 0\n",
        "\n",
        "        # Compute cross-entropy between smoothed labels and log probabilities\n",
        "        return -torch.mean(torch.sum(label_smoothed * log_probs, dim=-1))\n"
      ],
      "metadata": {
        "id": "qe63u9NwssEZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Learning Rate Scheduler for Transformer"
      ],
      "metadata": {
        "id": "HJzIi_77s2MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate scheduler based on Transformer warm-up formula from \"Attention is All You Need\"\n",
        "#Purpose of Learning Rate Scheduler\n",
        "#The scheduler adjusts the learning rate at each step or epoch, usually to:\n",
        "#Warm up (gradually increase) the learning rate at the start to prevent unstable updates.\n",
        "#Decay (gradually decrease) it later so the model can converge smoothly and fine-tune weights.\n",
        "\n",
        "class TransformerScheduler:\n",
        "    def __init__(self, optimizer, d_model, warmup_steps):\n",
        "        self.optimizer = optimizer       # Optimizer (e.g., Adam) whose LR will be updated\n",
        "        self.d_model = d_model           # Model dimensionality (used in scaling the LR)\n",
        "        self.warmup_steps = warmup_steps  # Number of steps to increase LR during warm-up\n",
        "        self.step_num = 0                # Counter to track the number of optimization steps\n",
        "\n",
        "    def step(self):\n",
        "        self.step_num += 1  # Increment the step count\n",
        "\n",
        "        # Compute learning rate based on:\n",
        "        # (1) Inverse square root of step number (after warm-up)\n",
        "        # (2) Linear increase during warm-up steps\n",
        "        # Final formula: (d_model)^(-0.5) * min(step_num^(-0.5), step_num * (warmup_steps)^(-1.5))\n",
        "        lr = (self.d_model ** -0.5) * min(self.step_num ** -0.5, self.step_num * self.warmup_steps ** -1.5)\n",
        "\n",
        "        # Update learning rate in the optimizer\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        # Return the current learning rate (optional for logging/monitoring)\n",
        "        return lr\n"
      ],
      "metadata": {
        "id": "YhQlw32Vs299"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Loop"
      ],
      "metadata": {
        "id": "eWkDlr3Uu3F7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Transformer model with hyperparameters and vocabulary sizes\n",
        "model = Transformer(d_model, n_heads, d_ff, max_seq_len, len(vocab_en), len(vocab_ka))\n",
        "\n",
        "# Adam optimizer with parameters recommended in the original Transformer paper\n",
        "#Function of Adam Optimizer. It updates the weights of the neural network during training to minimize the loss function by combining the advantages of two other optimizers:\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Custom learning rate scheduler based on warm-up strategy\n",
        "scheduler = TransformerScheduler(optimizer, d_model, warmup_steps)\n",
        "\n",
        "# Loss function with label smoothing to improve generalization\n",
        "criterion = CrossEntropyLossWithSmoothing(vocab_size_ka, label_smoothing)\n",
        "\n",
        "# Training function\n",
        "def train(model, data, epochs):\n",
        "    model.train()  # Set model to training mode (enable dropout, etc.)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for src, tgt in data:\n",
        "            # Convert input sequences to PyTorch tensors\n",
        "            src = torch.tensor([src], dtype=torch.long)\n",
        "\n",
        "            # Prepare decoder input by removing the <eos> token (last token)\n",
        "            tgt_in = torch.tensor([tgt[:-1]], dtype=torch.long)\n",
        "\n",
        "            # Prepare decoder target by removing the <sos> token (first token)\n",
        "            tgt_out = torch.tensor([tgt[1:]], dtype=torch.long)\n",
        "\n",
        "            # Create masks:\n",
        "            # src_mask: masks out padding tokens in source sequence\n",
        "            src_mask = create_padding_mask(src)\n",
        "\n",
        "            # tgt_mask: masks future positions for autoregressive decoding\n",
        "            tgt_mask = create_causal_mask(tgt_in.size(1))\n",
        "\n",
        "            # Zero gradients before backpropagation\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass through the model with masks\n",
        "            output = model(src, tgt_in, src_mask, tgt_mask)\n",
        "\n",
        "            # Calculate loss between model output and expected output\n",
        "            loss = criterion(output, tgt_out)\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to avoid exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Update model parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update learning rate according to scheduler\n",
        "            scheduler.step()\n",
        "\n",
        "            # Accumulate loss for reporting\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print average loss every 100 epochs for monitoring training progress\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {total_loss / len(data):.4f}\")\n",
        "\n",
        "# Start training loop\n",
        "train(model, data, epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA5heQVIu3mx",
        "outputId": "dfaaf393-05ac-4031-bc94-dca98adaa496"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 3.1706\n",
            "Epoch 100, Loss: 0.5654\n",
            "Epoch 200, Loss: 0.5650\n",
            "Epoch 300, Loss: 0.5649\n",
            "Epoch 400, Loss: 0.5649\n",
            "Epoch 500, Loss: 0.5649\n",
            "Epoch 600, Loss: 0.5649\n",
            "Epoch 700, Loss: 0.5649\n",
            "Epoch 800, Loss: 0.5649\n",
            "Epoch 900, Loss: 0.5649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation function from English to Kannada"
      ],
      "metadata": {
        "id": "WsByXi51u4TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translation function from English to Kannada using the trained Transformer model\n",
        "def translate(model, sentence, vocab_en, vocab_ka, max_len=10):\n",
        "    model.eval()  # Set model to evaluation mode (disable dropout, etc.)\n",
        "\n",
        "    # Create inverse vocabulary to map IDs back to words in Kannada\n",
        "    inv_vocab_ka = {v: k for k, v in vocab_ka.items()}\n",
        "\n",
        "    # Tokenize input sentence, add start-of-sequence (<sos>) and end-of-sequence (<eos>) tokens\n",
        "    tokens = [\"<sos>\"] + sentence.lower().split() + [\"<eos>\"]\n",
        "\n",
        "    # Convert tokens to their corresponding IDs, use 0 (usually <pad>) if token not found in vocab_en\n",
        "    src = [vocab_en.get(t, 0) for t in tokens]\n",
        "\n",
        "    # Pad the source sequence to max_len with zeros (padding token)\n",
        "    src = torch.tensor([src + [0] * (max_len - len(src))], dtype=torch.long)\n",
        "\n",
        "    # Create source mask to ignore padded tokens during attention\n",
        "    src_mask = create_padding_mask(src)\n",
        "\n",
        "    # Initialize target sequence with <sos> token followed by padding\n",
        "    tgt = torch.tensor([[vocab_ka[\"<sos>\"]] + [0] * (max_len - 1)], dtype=torch.long)\n",
        "\n",
        "    # Autoregressive decoding loop: predict tokens one-by-one until max_len or <eos> token generated\n",
        "    for i in range(max_len - 1):\n",
        "        # Create causal mask so decoder can't attend to future tokens beyond position i\n",
        "        tgt_mask = create_causal_mask(i + 1)\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient calculation during inference\n",
        "            # Forward pass: model predicts next token probabilities given current src and tgt tokens\n",
        "            output = model(src, tgt[:, :i + 1], src_mask, tgt_mask)\n",
        "\n",
        "        # Select the token with highest probability at the current position i\n",
        "        next_token = output[0, i, :].argmax().item()\n",
        "\n",
        "        # Append predicted token to target sequence\n",
        "        tgt[0, i + 1] = next_token\n",
        "\n",
        "        # Stop decoding if <eos> token is generated\n",
        "        if next_token == vocab_ka[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    # Convert predicted token IDs to words, excluding padding, <sos>, and <eos> tokens\n",
        "    translated_sentence = \" \".join(\n",
        "        inv_vocab_ka[t.item()]\n",
        "        for t in tgt[0]\n",
        "        if t.item() != 0 and t.item() != vocab_ka[\"<sos>\"] and t.item() != vocab_ka[\"<eos>\"]\n",
        "    )\n",
        "\n",
        "    return translated_sentence\n"
      ],
      "metadata": {
        "id": "dSKBiOEyu4--"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the Translator"
      ],
      "metadata": {
        "id": "GGTP2LrHu5av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set of English input sentences to evaluate the translation model\n",
        "test_sentences = [\"i go to school\", \"we eat food\", \"we go to school\", \"i read book\", \"we read book\"]\n",
        "\n",
        "# Loop through each sentence in the test set\n",
        "for sentence in test_sentences:\n",
        "    # Use the translate function to get Kannada translation\n",
        "    translated = translate(model, sentence, vocab_en, vocab_ka)\n",
        "\n",
        "    # Print original English sentence\n",
        "    print(f\"English: {sentence}\")\n",
        "\n",
        "    # Print corresponding Kannada translation generated by the model\n",
        "    print(f\"Kannada: {translated}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57_p11BOu6G0",
        "outputId": "b380e8a5-4a9e-4fe7-acbc-36e76370d4de"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: i go to school\n",
            "Kannada: naanu ge shale hoguttene\n",
            "\n",
            "English: we eat food\n",
            "Kannada: naavu ahara tinnuttene\n",
            "\n",
            "English: we go to school\n",
            "Kannada: naavu ge shale hoguttene\n",
            "\n",
            "English: i read book\n",
            "Kannada: naanu pustaka oduttene\n",
            "\n",
            "English: we read book\n",
            "Kannada: naavu pustaka oduttene\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation function from Kannada to English  "
      ],
      "metadata": {
        "id": "enlMrfFVEwFH"
      }
    }
  ]
}